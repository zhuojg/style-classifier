{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "style_classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFsbv3pbqIU8",
        "colab_type": "text"
      },
      "source": [
        "# Multi-class - Model & trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iFPf55A9EUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import datetime\n",
        "from torchvision import transforms, datasets\n",
        "import tqdm\n",
        "from torch.optim import SGD\n",
        "import torch.utils.data.dataset\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import BCELoss, CrossEntropyLoss\n",
        "import numpy as np\n",
        "import math\n",
        "from PIL import Image, ImageFile\n",
        "from google.colab import drive\n",
        "import os\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "\n",
        "class StyleClassifier(nn.Module):\n",
        "    def __init__(self, model, need_softmax=False):\n",
        "        super(StyleClassifier, self).__init__()\n",
        "\n",
        "        if model == 'vgg16':\n",
        "            self.pre_trained = models.vgg16(pretrained=True)\n",
        "            self.cnn = nn.Sequential(*list(self.pre_trained.children())[0])\n",
        "            self.conv_dr = nn.Conv2d(512, 128, 1)\n",
        "        elif model == 'vgg19':\n",
        "            self.pre_trained = models.vgg19(pretrained=True)\n",
        "            self.cnn = nn.Sequential(*list(self.pre_trained.children())[0])\n",
        "            self.conv_dr = nn.Conv2d(512, 128, 1)\n",
        "        elif model == 'resnet152':\n",
        "            self.pre_trained = models.resnet152(pretrained=True)\n",
        "            self.cnn = nn.Sequential(*list(self.pre_trained.children())[:-2])\n",
        "            self.conv_dr = nn.Conv2d(2048, 128, 1)\n",
        "        elif model == 'densenet169':\n",
        "            self.pre_trained = models.densenet169(pretrained=True)\n",
        "            self.cnn = nn.Sequential(*list(self.pre_trained.children())[0])\n",
        "            self.conv_dr = nn.Conv2d(1664, 128, 1)\n",
        "        else:\n",
        "            raise ValueError('model %s is not supported!' % (model))\n",
        "        \n",
        "        \n",
        "        self.fc = nn.Linear(in_features=128 * 128, out_features=1024)\n",
        "        self.fc_1 = nn.Linear(in_features=1024, out_features=4)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.need_softmax = need_softmax\n",
        "\n",
        "    def get_style_gram(self, style_features):\n",
        "        style_features = style_features.view(-1, 7 * 7, 128)\n",
        "        style_features_t = style_features.permute(0, 2, 1)\n",
        "        grams = []\n",
        "        for i in range(len(style_features)):\n",
        "#             normalize the gram matrix\n",
        "#             temp_result = np.matmul(style_features_t[i].cpu().detach().numpy(), style_features[i].cpu().detach().numpy())\n",
        "#             m = np.mean(temp_result, axis=0)\n",
        "#             std = np.std(temp_result, axis=0)\n",
        "\n",
        "#             grams.append((torch.matmul(style_features_t[i], style_features[i]) - torch.from_numpy(m).cuda()) / torch.from_numpy(std).cuda())\n",
        "            grams.append(torch.matmul(style_features_t[i], style_features[i]))\n",
        "\n",
        "        grams = torch.stack(grams, 0)\n",
        "        grams = grams.view(-1, 128 * 128)\n",
        "\n",
        "        return grams\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        h = self.cnn(h)\n",
        "        h = self.conv_dr(h)\n",
        "        \n",
        "        h = h.permute(0, 2, 3, 1)\n",
        "        h = self.get_style_gram(h)\n",
        "        \n",
        "        h = self.fc(h)\n",
        "        h = self.fc_1(h)\n",
        "\n",
        "        if self.need_softmax:\n",
        "            h = F.log_softmax(h, dim=1)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class ImageLoader(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, data_path, txt_path, is_train):\n",
        "        self.is_train = is_train\n",
        "        self.data = []\n",
        "        self.data_path = data_path\n",
        "        with open(txt_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                data = line.split(',')\n",
        "                self.data.append(data)\n",
        "\n",
        "        print(len(self.data))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.data[index][0], int(self.data[index][1])\n",
        "        img = Image.open(os.path.join(self.data_path, img))\n",
        "\n",
        "        img = img.convert('RGB')\n",
        "        \n",
        "        if self.is_train:\n",
        "            data_transforms = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.RandomCrop(224),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.RandomRotation(45),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "\n",
        "            img = data_transforms(img)\n",
        "        else:\n",
        "            data_transforms = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "\n",
        "            img = data_transforms(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, model, optimizer, train_loader, val_loader, out_path, max_iter):\n",
        "        self.model = model\n",
        "        self.opt = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.out_path = out_path\n",
        "        self.max_iter = max_iter\n",
        "        self.timestamp_start = datetime.datetime.now()\n",
        "\n",
        "        if not os.path.exists(self.out_path):\n",
        "            os.makedirs(self.out_path)\n",
        "\n",
        "        self.log_train_headers = [\n",
        "            'epoch',\n",
        "            'iteration',\n",
        "            'train/loss',\n",
        "            'train/acc',\n",
        "            'elapsed_time',\n",
        "        ]\n",
        "\n",
        "        self.log_val_headers = [\n",
        "            'epoch',\n",
        "            'iteration',\n",
        "            'val/loss',\n",
        "            'val/acc',\n",
        "            'elapsed_time',\n",
        "        ]\n",
        "\n",
        "        if not os.path.exists(os.path.join(self.out_path, 'log_train.csv')):\n",
        "            with open(os.path.join(self.out_path, 'log_train.csv'), 'w') as f:\n",
        "                f.write(','.join(self.log_train_headers) + '\\n')\n",
        "\n",
        "        if not os.path.exists(os.path.join(self.out_path, 'log_val.csv')):\n",
        "            with open(os.path.join(self.out_path, 'log_val.csv'), 'w') as f:\n",
        "                f.write(','.join(self.log_val_headers) + '\\n')\n",
        "\n",
        "        self.epoch = 0\n",
        "        self.iteration = 0\n",
        "\n",
        "    def validate(self):\n",
        "        training = self.model.training\n",
        "        self.model.eval()\n",
        "        val_loss = 0.\n",
        "        acc_all = []\n",
        "\n",
        "        for batch_idx, (img, label) in tqdm.tqdm(\n",
        "                enumerate(self.val_loader),\n",
        "                total=len(self.val_loader),\n",
        "                desc='Validation Epoch=%d' % self.epoch,\n",
        "                ncols=80,\n",
        "                leave=False\n",
        "        ):\n",
        "            img, label = Variable(img), Variable(label)\n",
        "            label = torch.tensor(label, dtype=torch.float32)\n",
        "            img, label = img.cuda(), label.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                result = self.model(img).cuda().squeeze(1)\n",
        "            \n",
        "            loss_fn = CrossEntropyLoss(weight=None, reduce=True)\n",
        "            loss = loss_fn(result, label.long())\n",
        "            val_loss += loss\n",
        "\n",
        "            acc = 0.\n",
        "            label = label.cpu()\n",
        "            result = result.cpu()\n",
        "            for index, item in enumerate(result):\n",
        "                if torch.max(item).item() == item[label.long()[index].item()].item():\n",
        "                    acc += 1\n",
        "\n",
        "            acc /= 16\n",
        "            acc_all.append(acc)\n",
        "\n",
        "        acc_all = np.array(acc_all)\n",
        "        print('Val Acc=%s' % (str(acc_all.mean())))\n",
        "\n",
        "        with open(os.path.join(self.out_path, 'log_val.csv'), 'a') as f:\n",
        "            elapsed_time = (datetime.datetime.now() - self.timestamp_start).total_seconds()\n",
        "            log = [self.epoch, self.iteration, val_loss.item(), acc_all.mean(), elapsed_time]\n",
        "            log = map(str, log)\n",
        "            f.write(','.join(log) + '\\n')\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': self.epoch,\n",
        "            'iteration': self.iteration,\n",
        "            'arch': self.model.__class__.__name__,\n",
        "            'optim_state_dict': self.opt.state_dict(),\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "        }, os.path.join(self.out_path, 'checkpoint.pth.tar'))\n",
        "\n",
        "        if training:\n",
        "            self.model.train()\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        epoch_loss = 0.\n",
        "        acc_all = []\n",
        "        \n",
        "        for batch_idx, (img, label) in tqdm.tqdm(\n",
        "                enumerate(self.train_loader),\n",
        "                total=len(self.train_loader),\n",
        "                desc='Train Epoch=%d' % self.epoch,\n",
        "                ncols=80,\n",
        "                leave=False\n",
        "        ):  \n",
        "            iteration = batch_idx + self.epoch * len(self.train_loader)\n",
        "            if self.iteration != 0 and (iteration - 1) != self.iteration:\n",
        "                continue\n",
        "            self.iteration = iteration\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "            img, label = Variable(img), Variable(label)\n",
        "            label = torch.tensor(label, dtype=torch.float32)\n",
        "            img, label = img.cuda(), label.cuda()\n",
        "\n",
        "            result = self.model(img).cuda().squeeze(1)\n",
        "            loss_fn = CrossEntropyLoss(weight=None, reduce=True)\n",
        "            loss = loss_fn(result, label.long())\n",
        "            try:\n",
        "                loss.backward()\n",
        "                self.opt.step()\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "\n",
        "            epoch_loss += loss.detach().cpu().numpy()\n",
        "\n",
        "            if self.iteration > 0 and self.iteration % 3 == 0:\n",
        "                acc = 0.\n",
        "                label = label.cpu()\n",
        "                result = result.cpu()\n",
        "                for index, item in enumerate(result):\n",
        "                    if torch.max(item).item() == item[label.long()[index].item()].item():\n",
        "                        acc += 1\n",
        "\n",
        "                acc /= 16\n",
        "                acc_all.append(acc)\n",
        "                print('Train Acc=%s' % str(np.array(acc_all).mean()))\n",
        "\n",
        "            if self.iteration >= self.max_iter:\n",
        "                break\n",
        "\n",
        "        with open(os.path.join(self.out_path, 'log_train.csv'), 'a') as f:\n",
        "            elapsed_time = (datetime.datetime.now() - self.timestamp_start).total_seconds()\n",
        "            log = [self.epoch, self.iteration, epoch_loss, np.array(acc_all).mean(), elapsed_time]\n",
        "            log = map(str, log)\n",
        "            f.write(','.join(log) + '\\n')\n",
        "\n",
        "    def train(self):\n",
        "        max_epoch = int(math.ceil(1. * self.max_iter / len(self.train_loader)))\n",
        "        for epoch in tqdm.trange(self.epoch, max_epoch, desc='Train', ncols=80):\n",
        "            self.epoch = epoch\n",
        "            self.train_epoch()\n",
        "            self.validate()\n",
        "            assert self.model.training\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    drive.mount('/gdrive')\n",
        "    \n",
        "    data_transforms_train = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    data_transforms_val = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    train_set = datasets.ImageFolder(root='/gdrive/My Drive/style_classifier/style_data_multiclass', transform=data_transforms_train)\n",
        "\n",
        "    val_set = datasets.ImageFolder(root='/gdrive/My Drive/style_classifier/style_data_multiclass_val', transform=data_transforms_val)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=16,\n",
        "        shuffle=True\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_set,\n",
        "        batch_size=16,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    model = StyleClassifier(model='densenet169').cuda()\n",
        "\n",
        "    opt = SGD(\n",
        "        model.parameters(),\n",
        "        lr=1e-4,\n",
        "        momentum=0.1\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        optimizer=opt,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        out_path='/gdrive/My Drive/style_classifier/log',\n",
        "        max_iter=100000\n",
        "    )\n",
        "\n",
        "    trainer.epoch = 0\n",
        "    trainer.iteration = 0\n",
        "    trainer.train()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NURLArakthQE",
        "colab_type": "text"
      },
      "source": [
        "# Style Classifier Demo (multi class)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAhJnCg9tkqa",
        "colab_type": "code",
        "outputId": "d5af445b-1566-447f-d661-25312a1daf95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "import os\n",
        "from torchvision import transforms, models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import gradio\n",
        "import numpy as np\n",
        "import gradio.preprocessing_utils\n",
        "import gradio.inputs\n",
        "import gradio.outputs\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "class StyleClassifier(nn.Module):\n",
        "    def __init__(self, need_softmax=False):\n",
        "        super(StyleClassifier, self).__init__()\n",
        "        self.pre_trained = models.vgg16(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(self.pre_trained.children())[0])\n",
        "        self.conv_dr = nn.Conv2d(512, 128, 1)\n",
        "        self.fc = nn.Linear(in_features=128 * 128, out_features=1024)\n",
        "        self.fc_1 = nn.Linear(in_features=1024, out_features=4)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.need_softmax = need_softmax\n",
        "\n",
        "    def get_style_gram(self, style_features):\n",
        "        style_features = style_features.view(-1, 7 * 7, 128)\n",
        "        style_features_t = style_features.permute(0, 2, 1)\n",
        "        grams = []\n",
        "        for i in range(len(style_features)):\n",
        "            grams.append(torch.matmul(style_features_t[i], style_features[i]))\n",
        "\n",
        "        grams = torch.stack(grams, 0)\n",
        "        grams = grams.view(-1, 128 * 128)\n",
        "\n",
        "        return grams\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        h = self.cnn(h)\n",
        "        h = self.conv_dr(h)\n",
        "        \n",
        "        h = h.permute(0, 2, 3, 1)\n",
        "        h = self.get_style_gram(h)\n",
        "        \n",
        "        h = self.fc(h)\n",
        "        h = self.fc_1(h)\n",
        "\n",
        "        if self.need_softmax:\n",
        "            h = F.log_softmax(h, dim=1)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "def random_data_generator(num):\n",
        "    result = []\n",
        "    f = open('/gdrive/My Drive/all.txt', 'r')\n",
        "\n",
        "    data = f.readlines()\n",
        "    random.shuffle(data)\n",
        "\n",
        "    for i, line in enumerate(data):\n",
        "        if i >= num:\n",
        "            break\n",
        "        result.append(line[:-1])\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def target_data_generator(target_tag, num):\n",
        "    result = []\n",
        "    f = open('/gdrive/My Drive/all.txt', 'r')\n",
        "\n",
        "    data = f.readlines()\n",
        "    random.shuffle(data)\n",
        "\n",
        "    for line in data:\n",
        "        if len(result) >= num:\n",
        "            break\n",
        "        if line.split('/')[0] == target_tag:\n",
        "            result.append(line[:-1])\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_confusion_matrix(y_pred, y_true, labels):\n",
        "    matrix = confusion_matrix(y_pred, y_true)\n",
        "    if len(matrix) != len(labels):\n",
        "        raise ValueError('The size of parameter labels[%s] is not equal to the size of labels in matrix[%s]'\n",
        "                         % (str(len(labels)), str(len(matrix))))\n",
        "    plt.matshow(matrix, cmap=plt.cm.Blues)\n",
        "    plt.title('True Label', fontsize=10)\n",
        "    plt.colorbar()\n",
        "    for x in range(len(matrix)):\n",
        "        for y in range(len(matrix)):\n",
        "            plt.annotate(matrix[x][y], xy=(y, x), horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "    plt.xticks(range(len(matrix)), labels)\n",
        "    plt.yticks(range(len(matrix)), labels)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.xaxis.set_ticks_position('top')\n",
        "\n",
        "    ax.set_ylabel('Predicted label')\n",
        "\n",
        "    return plt\n",
        "\n",
        "\n",
        "def test(pre_trained_path, labels, sample_per_label):\n",
        "    data = []\n",
        "    for item in labels:\n",
        "        data.extend(target_data_generator(item, sample_per_label))\n",
        "\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    model = StyleClassifier()\n",
        "\n",
        "    model.load_state_dict(torch.load(pre_trained_path)['model_state_dict'])\n",
        "\n",
        "    pred = []\n",
        "\n",
        "    for item in data:\n",
        "        im = Image.open(os.path.join('/gdrive/My Drive/style_data_clean', item))\n",
        "        im = im.convert('RGB')\n",
        "\n",
        "        predict = model(data_transforms(im).unsqueeze(0)).tolist()[0]\n",
        "\n",
        "        max_index = 0\n",
        "        temp_max = predict[0]\n",
        "        for i in range(1, len(predict)):\n",
        "            if predict[i] > temp_max:\n",
        "                temp_max = predict[i]\n",
        "                max_index = i\n",
        "\n",
        "        pred.append(max_index)\n",
        "\n",
        "    data_dict = {}\n",
        "    for i, item in enumerate(labels):\n",
        "        data_dict[item] = i\n",
        "\n",
        "    for i, _ in enumerate(data):\n",
        "        data[i] = data_dict[data[i].split('/')[0]]\n",
        "\n",
        "    get_confusion_matrix(pred, data, labels).show()\n",
        "\n",
        "\n",
        "def image_process(inp):\n",
        "    im = gradio.preprocessing_utils.decode_base64_to_image(inp)\n",
        "    im = im.convert('RGB')\n",
        "    data_transforms_val = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    im = data_transforms_val(im).unsqueeze(0)\n",
        "\n",
        "    return im.numpy()\n",
        "\n",
        "\n",
        "def web_demo(model):\n",
        "    inp = gradio.inputs.ImageUpload(preprocessing_fn=image_process)\n",
        "    out = gradio.outputs.Label(label_names=['lively', 'modern', 'pop_art', 'vintage'], num_top_classes=4)\n",
        "    io = gradio.Interface(inputs=inp, outputs=out, model_type=\"pytorch\", model=model)\n",
        "    io.launch(inline=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    drive.mount('/gdrive')\n",
        "    pre_trained_model = '/gdrive/My Drive/style_classifier/log/190731_multi.pth.tar'\n",
        "    \n",
        "    # get 50 images for each label and draw confusion matrix\n",
        "    test(pre_trained_model, ['lively', 'modern', 'pop_art', 'vintage'], 50)\n",
        "\n",
        "    # interactive demo using Gradio\n",
        "    model = StyleClassifier(need_softmax=True)\n",
        "    model.load_state_dict(torch.load(pre_trained_model)['model_state_dict'])\n",
        "    web_demo(model)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASYAAAD0CAYAAAA2RYtKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOZc4npicc4nz/6ra8hAZor+7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}